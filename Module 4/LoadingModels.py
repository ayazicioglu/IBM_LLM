#Setup
    #We need the following libraries:

    # torch             for deep learning and neural network modeling.
    # transformers      for accessing pretrained models and performing various NLP tasks with ease.

#Installing required libraries (cmd)
    # !pip install torch 
    # !pip install transformers

#Importing required libraries
from transformers import pipeline
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')


# Text classification with DistilBERT
# Load the model and tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# Preprocess the input text
# Sample text
text = "Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim."

# Tokenize the input text
inputs = tokenizer(text, return_tensors="pt")

print(inputs)

"""Perform inference
The torch.no_grad() context manager is used to disable gradient calculation. This reduces memory consumption and speeds up computation, as gradients are not needed for inference (i.e. when you are not training the model). 
The **inputs syntax is used to unpack a dictionary of keyword arguments in Python. In the context of the model(**inputs):"""

# Perform inference
with torch.no_grad():
    outputs = model(**inputs)

"""Perform inference
The torch.no_grad() context manager is used to disable gradient calculation. This reduces memory consumption and speeds up computation, as gradients are not needed for inference (i.e. when you are not training the model). 
The **inputs syntax is used to unpack a dictionary of keyword arguments in Python. In the context of the model(**inputs):"""

# Perform inference
with torch.no_grad():
    outputs = model(**inputs)